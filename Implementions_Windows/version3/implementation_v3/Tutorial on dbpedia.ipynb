{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# ðŸ“– KeyClass Tutorial: Text Classification with Label-Descriptions Only\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "***Author(s):*** Arnab Dey, Chufan Gao, Mononito Goswami, correspondence to &lt;mgoswami@andrew.cmu.edu&gt;\n",
    "\n",
    "<img align=\"right\" src=\"../assets/autonlab_logo.png\" width=\"20%\"/>\n",
    "\n",
    "## Contents\n",
    "\n",
    "\n",
    "### 1. [Problem Background & Motivation](#introduction) \n",
    "\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  1.1 [Electronic Health Records (EHR)](#ehr)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  1.2 [Generalizable Insights in Healthcare Contexts](#insights)\n",
    "\n",
    "\n",
    "### 2. [Methodology](#methodology) \n",
    "\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.1 [Prior Work](#prior)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.2 [KeyClass](#keyclass)\n",
    "<!-- ####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.3 [Problem Formulation](#math) -->\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.3 [Find Class Descriptions](#classdesc)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.4 [Find Relevant Keywords](#keywords)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.5 [Probabilistically Labeling the Data](#label)\n",
    "\n",
    "\n",
    "### 3. [Experimentation: Training](#exp_training) \n",
    "\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  3.1 [Training the Downstream Model](#downstream)\n",
    "####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  3.2 [Self-Training the Model](#self)\n",
    "\n",
    "### 4. [Experimentation: Testing](#exp_testing) \n",
    "\n",
    "### 5. [References](#references) \n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1. Problem Background & Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ehr'></a>\n",
    "### 1.1 Electronic Health Records (EHR) \n",
    "\n",
    "The Electronic Health Record __(EHR)__ system is a digital version of a patientâ€™s paper chart. __EHRs__ are almost-real-time, patient-centered records that contain `patient history`, `diagnoses`, `procedures`, `medications`, and more in an easily accessible format. Since the _Health Information Technology for Economic and Clinical Health_ <b>(HITECH)</b> Act was signed into law in 2009, adoption rates of these systems have steadily increased<sup><a href=\"#references\"><b>1</b></a></sup>. Adler-Milstein et al.<sup><a href=\"#references\"><b>2</b></a></sup>, who analyzed survey data collected by American Hospital Association found that EHR adoption rates were at <code><b>80%</b></code> in __2017__, twice the rate in __2008__. With higher adoption rates comes the rising challenge of _data processing and analysis of unstructured clinical text._ Due to the unstructured nature of clinical notes, providers often employ trained staff and/or third-party vendors to help assign diagnostic codes using coding systems such as the International Classification of Diseases __(ICD)__<sup><a href=\"#references\"><b>3</b></a></sup>. \n",
    "\n",
    "__However, manual assignment of ICD codes is problematic:__\n",
    "1. It is both time consuming and error-prone, with only <code><b>60-80%</b></code> of the assigned codes reflecting actual patient diagnoses<sup><a href=\"#references\"><b>4</b></a></sup>\n",
    "1. A significant portion of code assignment results in misjudged severity of conditions and code omissions<sup><a href=\"#references\"><b>5</b></a></sup>\n",
    "1. For healthcare providers, billing and coding errors may not only lead to loss of revenue and claim denials, but also federal penalties for erroneous Medicare and Medicaid claims\n",
    "\n",
    "Thus, there is a clear need for reliable automated classification of unstructured clinical notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='insights'></a>\n",
    "### 1.2 Generalizable Insights in Healthcare Contexts \n",
    "\n",
    "Managing costs and quality of healthcare is a persistent societal challenge of enormous magnitude and impact on daily lives of all people. Our approach proposes a low-cost solution that has the potential to address some of the identified pressing issues with accessibility to affordable yet accurate automated disease coding tools. Our contributions lie in using a novel strategy\n",
    "to efficiently acquire interpretable weak supervision sources from readily available text to learn effective text classifiers without the need for human-labeled data.\n",
    "\n",
    "__Our work demonstrates:__\n",
    "1. Pre-trained language models can efficiently and effectively inform weakly supervised models for text classification\n",
    "1. Self-training improves downstream classifier performance, especially when classifiers are initially trained on a subset of the training data\n",
    "1. Data programming performs on par with simple majority vote when relying on a large number of automatically generated weak supervision sources of similar quality\n",
    "1. Keywords are excellent sources of weak supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='methodology'></a>\n",
    "## 2. Methodology \n",
    "\n",
    "<!-- <center> -->\n",
    "<img align=\"top\" src=\"../assets/KeyClass.png\" width=\"50%\"/>\n",
    "<!-- </center> -->\n",
    "\n",
    "<b>Figure A:</b> Overview of our methodology. From only class descriptions, KeyClass classifies documents without access to any labeled data. It automatically creates interpretable labeling functions (LFs) by extracting frequent keywords and phrases that are highly indicative of a particular class from the unlabeled text using a pre-trained language model. It then uses these LFs along with Data Programming (DP) to generate probabilistic labels for training data, which are used to train a downstream classifier <sup><a href=\"#references\"><b>13</b></a></sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prior'></a>\n",
    "### 2.1 Prior Work\n",
    "\n",
    "__Assigning ICD codes to Clinical Notes<sup><a href=\"#references\"><b>[6,7,8]</b></a></sup>:__\n",
    "1. To the best of our knowledge, all prior work on ICD code assignment utilized __fully supervised ML techniques__, most of them relying on vast quantities of labeled training data\n",
    "1. In this work, we explore the use of our proposed __weakly supervised model _KeyClass___ to assign top-level `ICD-9` codes to long patient discharge summaries\n",
    "1. Its training signal is retrieved automatically from readily available descriptions of the ICD codes, therefore it requires no human-produced supervisory feedback to build effective downstream text classifiers\n",
    "\n",
    "__Text Classification with Sparse Training Labels<sup><a href=\"#references\"><b>[9,10]</b></a></sup>:__\n",
    "1. Our work differs from prior work because the foundation of our weak supervision methodology, i.e., frequent keywords and phrases as LFs, is highly interpretable\n",
    "1. Secondly, while previously proposed state-of-the-art models are committed to specific language model architectures for linguistic knowledge and representation learning, KeyClass offers a high degree of modularity, enabling end users to adapt the neural language model (encoder) and downstream classifiers to specific problems, such as clinical text classification\n",
    "1. Finally, we explore the use of weak supervision for multilabel multiclass classification, a problem which, to the best of our knowledge, has not been tackled by prior work on weak text classification\n",
    "\n",
    "__Weak Supervision for Clinical Text Classification<sup><a href=\"#references\"><b>[11,12]</b></a></sup>:__\n",
    "1. Prior work on weakly supervised clinical text classification had an explicit dependence on manually created rule-based labeling functions\n",
    "1. In this work, however, we demonstrate that we can quickly and automatically create simple keyword based labeling functions, with minimal to no human involvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='keyclass'></a>\n",
    "### 2.2 KeyClass\n",
    "\n",
    "As a potential remedy, we present KeyClass, a general weakly supervised text classification framework combining Data Programming<sup><a href=\"#references\"><b>13</b></a></sup> with a novel method of automatically acquiring interpretable weak supervision sources (keywords and phrases) from class-label descriptions only without the need to access to any labeled documents. The successful application of KeyClass to solve an important clinical text classification problem demonstrates its potential for making social impact by allowing quick and affordable development and deployment of effective text classifiers.\n",
    "\n",
    "<img align=\"top\" src=\"../assets/flowchart.png\" width=\"50%\"/>\n",
    "\n",
    "<b>Figure B:</b> Data programming, or weak supervision compared to fully supervised ML. The orange boxes indicate the effort required by expert annotators. Instead of having to label extensive quantities of data by hand, the effort in data programming framework lies in obtaining labeling functions. In KeyClass, these labeling functions are our keyword-matching rules automatically extracted from reference data, to further reduce required human effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classdesc'></a>\n",
    "### 2.3 Find Class Descriptions\n",
    "Unlike traditional supervised learning where each document needs to be labeled, KeyClass only relies on meaningful and succinct class descriptions, also removing the requirement of expert heuristics as in prior weak supervision work. As a concrete example, let's consider the IMDb movie review sentiment classification problem, where the objective is to classify a movie re-\n",
    "view as being `positive` or `negative`. In order to initiate the classification process, domain experts provide <code><b>KeyClass</b></code> with common sense descriptions of a __positive__ (`good amazing exciting positive`) and __negative review__ (`terrible bad boring negative`). In most cases, these descriptions can be automatically generated from Wikipedia articles or reference manuals and validated by domain experts, further reducing manual effort. Class Descriptions used in this tutorial can be found [here](./config_files/config_imdb.yml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='keywords'></a>\n",
    "### 2.4 Find Relevant Keywords / Encoding the Dataset\n",
    "\n",
    "Once we have the class descriptions, KeyClass automatically discovers highly suggestive keywords and phrases for each class. KeyClass first obtains frequent n-grams from the training corpus to serve as keywords or key-phrases for its automatically composed labeling functions. In order to transform the keywords into labeling functions of the prescribed form, KeyClass leverage the general linguistic knowledge stored within pre-trained neural language models such as Bidirectional Encoder Representations from Transformers __(BERT)__<sup><a href=\"#references\"><b>14</b></a></sup>, to map each keyword to the most semantically related category description. To create a labeling function, KeyClass simply assigns a keyword to its closest category as measured by the cosine similarity between their embeddings. In order to ensure equal representation of all classes, KeyClass sub-samples the top-k labeling functions per class, ordering them by cosine similarity. While theoretically data programming benefits from as many labeling functions as possible, the sampling is required due to computational and space constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/beast/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../keyclass/')\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "import argparse\n",
    "import label_data, encode_datasets, train_downstream_model\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join, exists\n",
    "from datetime import datetime\n",
    "import utils\n",
    "import models\n",
    "import create_lfs\n",
    "import train_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "config_file_path = r'../config_files/config_dbpedia.yml' # Specify path to the configuration file\n",
    "random_seed = 0 # Random seed for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = utils.Parser(config_file_path=config_file_path).parse()\n",
    "# model_name = \"bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16\"\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d69a33c7ea84fbba1e454e379bfaa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0284f0ae20df44bea5e7b8d9820f6a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if args['use_custom_encoder']:\n",
    "    model = models.CustomEncoder(pretrained_model_name_or_path=args['base_encoder'], \n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    model = models.Encoder(model_name=args['base_encoder'], \n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    sentences = utils.fetch_data(dataset=args['dataset'], split=split, path=args['data_path'])\n",
    "    embeddings = model.encode(sentences=sentences, batch_size=args['end_model_batch_size'], \n",
    "                                show_progress_bar=args['show_progress_bar'], \n",
    "                                normalize_embeddings=args['normalize_embeddings'])\n",
    "    with open(join(args['data_path'], args['dataset'], f'{split}_embeddings.pkl'), 'wb') as f:\n",
    "        pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.40.0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/beast/anaconda3/envs/keyclass/lib/python3.8/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: sentence-transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='label'></a>\n",
    "### 2.5 Probabilistically Labeling the Data\n",
    "\n",
    "Next, _KeyClass_ constructs the labeling function vote matrix and generates probabilistic labels for all training documents using a label model. Specifically, we use the open-source label model implementation of the ___Snorkel Python library___<sup><a href=\"#references\"><b>13</b></a></sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting labels for the dbpedia data...\n",
      "Size of the data: 168000\n",
      "Class distribution (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]), array([27295, 13534,  6701, 10242,  9408,  3931, 12525, 12232, 13455,\n",
      "       12205,  7375, 13151, 12713, 13233]))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load training data\n",
    "train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n",
    "\n",
    "training_labels_present = False\n",
    "if exists(join(args['data_path'], args['dataset'], 'train_labels.txt')):\n",
    "    with open(join(args['data_path'], args['dataset'], 'train_labels.txt'), 'r') as f:\n",
    "        y_train = f.readlines()\n",
    "    y_train = np.array([int(i.replace('\\n','')) for i in y_train])\n",
    "    training_labels_present = True\n",
    "else:\n",
    "    y_train = None\n",
    "    training_labels_present = False\n",
    "    print('No training labels found!')\n",
    "\n",
    "with open(join(args['data_path'], args['dataset'], 'train_embeddings.pkl'), 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"Getting labels for the {args['dataset']} data...\")\n",
    "print(f'Size of the data: {len(train_text)}')\n",
    "if training_labels_present:\n",
    "    print('Class distribution', np.unique(y_train, return_counts=True))\n",
    "\n",
    "# Load label names/descriptions\n",
    "label_names = []\n",
    "for a in args:\n",
    "    if 'target' in a: label_names.append(args[a])\n",
    "\n",
    "\n",
    "# model_name = \"bluebert_pubmed_uncased_L-24_H-1024_A-16\"\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Creating labeling functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using huggingface encoder\n"
     ]
    }
   ],
   "source": [
    "labeler = create_lfs.CreateLabellingFunctions(base_encoder=args['base_encoder'],\n",
    "                                            device=torch.device(args['device']),\n",
    "                                            label_model=args['label_model'], custom_encoder=args['use_custom_encoder'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found assigned category counts [670 370 262 291 365 260 521 261 607 129 141 527 183 225]\n",
      "labeler.vocabulary:\n",
      " 4812\n",
      "labeler.word_indicator_matrix.shape (168000, 518)\n",
      "Len keywords 518\n",
      "assigned_category: Unique and Counts (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]), array([37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37]))\n",
      "company ['brand' 'brands' 'business' 'businesses' 'businessman' 'ceo' 'co ltd'\n",
      " 'commercial' 'companies' 'company' 'company based' 'company founded'\n",
      " 'company headquartered' 'corp' 'corporate' 'corporation' 'customers'\n",
      " 'employees' 'entrepreneur' 'establishment' 'executive' 'factory' 'firm'\n",
      " 'industries' 'industry' 'management' 'manufacturer' 'manufactures'\n",
      " 'manufacturing' 'manufacturing company' 'multinational' 'organisation'\n",
      " 'owners' 'production company' 'provider' 'publishing company' 'retailer']\n",
      "school, university ['campus' 'campuses' 'college located' 'college preparatory' 'colleges'\n",
      " 'educational institution' 'elementary school' 'high school'\n",
      " 'high school located' 'high schools' 'preparatory school'\n",
      " 'public high school' 'public school' 'public secondary school'\n",
      " 'public university' 'school' 'school also' 'school high'\n",
      " 'school high school' 'school located' 'school named' 'school offers'\n",
      " 'school part' 'school private' 'school public high' 'school school'\n",
      " 'school secondary' 'school secondary school' 'school serves' 'school st'\n",
      " 'schools' 'secondary school' 'secondary school located' 'universities'\n",
      " 'university' 'university college' 'university located']\n",
      "artist ['american musician' 'american rapper' 'american singer songwriter' 'art'\n",
      " 'artist' 'artists' 'arts' 'comic' 'composer' 'creative' 'creator'\n",
      " 'depicts' 'designer' 'drawn' 'graphic' 'illustrator' 'image'\n",
      " 'italian painter' 'music artist' 'music producer' 'music singer'\n",
      " 'musician' 'painted' 'painter' 'painting' 'paintings' 'performer'\n",
      " 'pictures' 'poet' 'pop singer' 'rapper' 'recording artist' 'sculptor'\n",
      " 'singer' 'singer songwriter' 'songwriter' 'vocalist']\n",
      "athlete ['athlete' 'athletic' 'athletics' 'basketball' 'basketball player' 'boxer'\n",
      " 'champion' 'championships' 'coach' 'cricketer' 'football'\n",
      " 'football player' 'football player currently' 'footballer'\n",
      " 'former professional' 'goalkeeper' 'hockey player' 'olympic' 'olympics'\n",
      " 'player' 'player currently playing' 'player played' 'player plays'\n",
      " 'players' 'professional basketball' 'professional basketball player'\n",
      " 'professional football player' 'professional footballer'\n",
      " 'rules footballer' 'runner' 'sport' 'sports' 'swimmer' 'tennis'\n",
      " 'tennis player' 'trainer' 'wrestler']\n",
      "politics ['american politician' 'australian politician' 'canadian politician'\n",
      " 'capitol' 'congress' 'congressional' 'constituency' 'democrat'\n",
      " 'democratic' 'democratic member' 'elected' 'election' 'elections'\n",
      " 'government' 'lawyer politician' 'legislative' 'legislature'\n",
      " 'member parliament' 'parliament' 'party politician' 'policy' 'political'\n",
      " 'political figure' 'political party' 'politician' 'politician born'\n",
      " 'politician former' 'politician member' 'politician republican'\n",
      " 'politician served' 'politics' 'republican' 'republican member'\n",
      " 'republican party' 'senate' 'senate representing' 'senator']\n",
      "transportation ['airline' 'airlines' 'airport' 'automobile' 'bus' 'cargo' 'carrier'\n",
      " 'driver' 'escort' 'express' 'ferry' 'freight' 'highway' 'journey'\n",
      " 'locomotive' 'locomotives' 'metro' 'passenger' 'passengers' 'rail'\n",
      " 'railroad' 'railway' 'railways' 'route' 'routes' 'shipping' 'steamship'\n",
      " 'tracks' 'train' 'trains' 'transit' 'transport' 'transportation' 'travel'\n",
      " 'van' 'vehicle' 'vehicles']\n",
      "building ['architect' 'architects' 'architectural' 'architecture' 'build'\n",
      " 'building' 'building built' 'building constructed' 'building located'\n",
      " 'buildings' 'built' 'church building' 'church built' 'constructed'\n",
      " 'construction' 'contributing buildings' 'designed built' 'dwelling'\n",
      " 'facilities' 'facility' 'historic building' 'house' 'house built'\n",
      " 'house historic' 'house located' 'houses' 'housing' 'ii listed building'\n",
      " 'listed building' 'office building' 'originally built' 'roof'\n",
      " 'school building' 'skyscraper' 'structures' 'tower' 'towers']\n",
      "river, mountain, lake ['alpine' 'aquatic' 'creek' 'elevations' 'freshwater' 'headwaters'\n",
      " 'highlands' 'km tributary' 'lake' 'lake lake' 'lake located' 'lakes'\n",
      " 'landscape' 'lowland' 'marshes' 'mountain' 'mountain located'\n",
      " 'mountain range' 'mountains' 'natural habitats' 'nature' 'reservoir'\n",
      " 'river' 'river flows' 'river north' 'river river' 'river romania'\n",
      " 'river tributary' 'rivers' 'stream' 'tributaries' 'tributary' 'valley'\n",
      " 'water' 'waters' 'watershed' 'wilderness']\n",
      "village ['2002 census village' 'borough' 'census village'\n",
      " 'census village population' 'colony' 'commune' 'communities' 'community'\n",
      " 'country' 'history pomerania village' 'households' 'located village'\n",
      " 'municipality' 'neighborhood' 'pomerania village'\n",
      " 'pomerania village population' 'residential' 'residents' 'rural'\n",
      " 'rural district' 'settlement' 'suburb' 'suburbs' 'town' 'towns'\n",
      " 'township' 'tribe' 'village' 'village administrative'\n",
      " 'village administrative district' 'village development'\n",
      " 'village development committee' 'village district' 'village located'\n",
      " 'village municipality' 'village population' 'villages']\n",
      "animal ['animal' 'animals' 'bark' 'bear' 'bears' 'bird' 'buffalo'\n",
      " 'containing following species' 'contains following species'\n",
      " 'county pomeranian' 'county west pomeranian' 'dog' 'feed' 'fish'\n",
      " 'following species' 'fox' 'frog' 'habitat' 'horse' 'hunter'\n",
      " 'kuyavian pomeranian' 'mare' 'pomeranian' 'racehorse' 'snake' 'snout'\n",
      " 'species' 'species bird' 'species frog' 'species genus' 'species snout'\n",
      " 'subspecies' 'thoroughbred' 'two species' 'west pomeranian' 'wild' 'wolf']\n",
      "plant, tree ['botanist' 'branches' 'cultivar' 'flowering' 'flowering plant'\n",
      " 'flowering plant family' 'flowering plants' 'flowering plants daisy'\n",
      " 'forest' 'forests' 'garden' 'genus flowering' 'genus flowering plants'\n",
      " 'herb' 'host plant' 'leaf' 'leaves' 'perennial herb' 'plant'\n",
      " 'plant family' 'plant genus' 'plant native' 'plant species' 'plants'\n",
      " 'plants daisy' 'plants daisy family' 'plants family' 'seeds' 'shrub'\n",
      " 'shrubland' 'shrubs' 'small tree' 'species flowering'\n",
      " 'species flowering plant' 'species plant' 'tree' 'trees']\n",
      "album ['album' 'album american' 'album band' 'album british' 'album canadian'\n",
      " 'album english' 'album features' 'album first' 'album produced'\n",
      " 'album recorded' 'album released' 'albums' 'albums chart'\n",
      " 'compilation album' 'debut album' 'debut studio album' 'first album'\n",
      " 'fourth album' 'fourth studio album' 'full length album' 'length album'\n",
      " 'live album' 'music' 'music album' 'music group' 'record label'\n",
      " 'records album' 'records label' 'second album' 'second studio album'\n",
      " 'solo album' 'songs' 'soundtrack' 'studio album' 'studio album released'\n",
      " 'third album' 'third studio album']\n",
      "film ['action film' 'american film' 'cinema' 'documentary' 'documentary film'\n",
      " 'documentary film directed' 'drama film' 'feature film' 'film'\n",
      " 'film also' 'film based' 'film directed' 'film director' 'film features'\n",
      " 'film festival' 'film made' 'film premiered' 'film produced'\n",
      " 'film released' 'film starring' 'film stars' 'film written' 'filmed'\n",
      " 'filmmaker' 'films' 'first film' 'hollywood' 'international film'\n",
      " 'international film festival' 'language film' 'motion picture' 'movie'\n",
      " 'movies' 'roles film' 'short film' 'silent film' 'television film']\n",
      "novel, publication, book ['american author' 'american writer' 'author' 'authors' 'based novel'\n",
      " 'book' 'book published' 'book series' 'book written' 'books' 'fiction'\n",
      " 'fiction book' 'fiction novel' 'first book' 'first novel'\n",
      " 'first published' 'genre' 'literary' 'literature' 'magazine published'\n",
      " 'newspaper published' 'novel' 'novel american' 'novel name'\n",
      " 'novel written' 'novelist' 'novels' 'paperback' 'publication'\n",
      " 'publications' 'published' 'publisher' 'publishers' 'publishes'\n",
      " 'publishing' 'writer' 'writers']\n",
      "==== Training the label model ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "INFO:root:Using GPU...\n",
      "  0%|          | 0/100 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.035]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:38<00:00,  1.59s/epoch]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "proba_preds = labeler.get_labels(text_corpus=train_text, label_names=label_names, min_df=args['min_df'], \n",
    "                                ngram_range=args['ngram_range'], topk=args['topk'], y_train=y_train, \n",
    "                                label_model_lr=args['label_model_lr'], label_model_n_epochs=args['label_model_n_epochs'], \n",
    "                                verbose=True, n_classes=args['n_classes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proba_preds [[0.03488939 0.03291201 0.02717606 ... 0.02513903 0.02480605 0.02601135]\n",
      " [0.07142857 0.07142857 0.07142857 ... 0.07142857 0.07142857 0.07142857]\n",
      " [0.02357292 0.35400056 0.14938836 ... 0.007827   0.0085955  0.01097113]\n",
      " ...\n",
      " [0.03488939 0.03291201 0.02717606 ... 0.02513903 0.02480605 0.02601135]\n",
      " [0.00168724 0.00303003 0.00120215 ... 0.00116112 0.00129731 0.00139847]\n",
      " [0.02747391 0.0340891  0.02689659 ... 0.02473755 0.0246471  0.02636079]]\n"
     ]
    }
   ],
   "source": [
    "print('proba_preds', proba_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "y_train_pred = np.argmax(proba_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = torch.nn.functional.one_hot(torch.tensor(y_train_pred), num_classes=14).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48951/3567368297.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.nn.functional.one_hot(torch.tensor(y_train).clone().detach(), num_classes=14).float()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": [
    "y_train = torch.nn.functional.one_hot(torch.tensor(y_train), num_classes=14).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp hot one encoding for y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "if not os.path.exists(args['preds_path']): os.makedirs(args['preds_path'])\n",
    "with open(join(args['preds_path'], f\"{args['label_model']}_proba_preds.pkl\"), 'wb') as f:\n",
    "    pickle.dump(proba_preds, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(args['data_path'], args['dataset'], 'train_labels_.txt'), 'w') as f:\n",
    "    for item in y_train_pred:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 0 1 ... 6 6 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model Predictions: Unique value and counts (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]), array([27295, 13534,  6701, 10242,  9408,  3931, 12525, 12232, 13455,\n",
      "       12205,  7375, 13151, 12713, 13233]))\n",
      "Label Model Training Accuracy 1.0\n",
      "Saving results in ../results/dbpedia/train_label_model_with_ground_truth_07-May-2024-22_18_47.txt...\n"
     ]
    }
   ],
   "source": [
    "# Print statistics\n",
    "print('Label Model Predictions: Unique value and counts', np.unique(y_train_pred, return_counts=True))\n",
    "if training_labels_present:\n",
    "    print('Label Model Training Accuracy', np.mean(y_train_pred==y_train))\n",
    "\n",
    "    # Log the metrics\n",
    "    training_metrics_with_gt = utils.compute_metrics(y_preds=y_train_pred, y_true=y_train, average=args['average'])\n",
    "    utils.log(metrics=training_metrics_with_gt, filename='label_model_with_ground_truth', \n",
    "        results_dir=args['results_path'], split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exp_training'></a>\n",
    "## 3. Experimentation: Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='downstream'></a>\n",
    "### 3.1 Training the Downstream Model\n",
    "\n",
    "After obtaining a probabilistically labeled training dataset, KeyClass can train any downstream classifier using rich document feature representations provided by the neural language model. Instead of using all the automatically labeled documents, KeyClass initially trains the downstream classifier using top-$k$ documents with the most confident label estimates only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence of least confident data point of class 0: 0.07142857142857144\n",
      "Confidence of least confident data point of class 1: 0.07142857142857144\n",
      "Confidence of least confident data point of class 2: 0.07142857142857144\n",
      "Confidence of least confident data point of class 3: 0.07142857142857144\n",
      "Confidence of least confident data point of class 4: 0.07142857142857144\n",
      "Confidence of least confident data point of class 5: 0.07142857142857144\n",
      "Confidence of least confident data point of class 6: 0.07142857142857144\n",
      "Confidence of least confident data point of class 7: 0.07142857142857144\n",
      "Confidence of least confident data point of class 8: 0.07142857142857144\n",
      "Confidence of least confident data point of class 9: 0.07142857142857144\n",
      "Confidence of least confident data point of class 10: 0.07142857142857144\n",
      "Confidence of least confident data point of class 11: 0.07142857142857144\n",
      "Confidence of least confident data point of class 12: 0.07142857142857144\n",
      "Confidence of least confident data point of class 13: 0.07142857142857144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beast/Documents/DeepLearning/Final/KeyClassReproducibility/tutorials/../scripts/train_downstream_model.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(return_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Data statistics ====\n",
      "Size of training data: (168000, 768), testing data: (14000, 768)\n",
      "Size of testing labels: (14000,)\n",
      "Size of training labels: (168000,)\n",
      "Training class distribution (ground truth): [0.48035119 0.16247024 0.235      0.04389881 0.07827976]\n",
      "Training class distribution (label model predictions): [13.44307143  0.55692857]\n",
      "\n",
      "KeyClass only trains on the most confidently labeled data points! Applying mask...\n",
      "\n",
      "==== Data statistics (after applying mask) ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: paraphrase-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: (166578, 768)\n",
      "Size of training labels: (166578,)\n",
      "Training class distribution (ground truth): [0.48445173 0.15532063 0.23700609 0.04427355 0.078948  ]\n",
      "Training class distribution (label model predictions): [13.43831718  0.56168282]\n",
      "X_train_embed_masked [[-0.12918349  0.14052801 -0.04047182 ...  0.12626587 -0.07761192\n",
      "  -0.13966799]\n",
      " [ 0.05691513 -0.00282671 -0.06619749 ...  0.0996549  -0.04426401\n",
      "  -0.0915379 ]\n",
      " [ 0.07235474 -0.13375334  0.03174148 ...  0.15466037 -0.08433446\n",
      "  -0.0442021 ]\n",
      " ...\n",
      " [ 0.07823674  0.02514957  0.02553626 ...  0.14149691 -0.05986933\n",
      "  -0.1253306 ]\n",
      " [-0.02489712 -0.02762321  0.08708651 ...  0.11298889 -0.00302183\n",
      "  -0.02341386]\n",
      " [-0.13798836  0.13346711  0.03419797 ...  0.0331003  -0.28610492\n",
      "   0.12628438]] \n",
      "\n",
      "y_train_lm_masked [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] \n",
      "\n",
      "y_train_masked [list([]) list([0]) list([1]) ... list([]) list([]) list([])] \n",
      "\n",
      "X_test_embed [[-0.1570467  -0.12320479 -0.02813797 ...  0.04563895  0.1186905\n",
      "  -0.019365  ]\n",
      " [ 0.06584813 -0.03715748  0.01288924 ... -0.00259125 -0.07262005\n",
      "  -0.03180559]\n",
      " [-0.10970274  0.07114383  0.0250446  ...  0.02382419  0.16102909\n",
      "  -0.08448644]\n",
      " ...\n",
      " [-0.02273611 -0.15575673 -0.06795695 ... -0.00937154 -0.08806276\n",
      "  -0.13163234]\n",
      " [ 0.02494795  0.05408705  0.03013977 ...  0.18539888 -0.09376797\n",
      "  -0.02945442]\n",
      " [ 0.02713903 -0.17202401 -0.09361733 ... -0.00501621 -0.05071437\n",
      "   0.04435562]] \n",
      "\n",
      "y_test [list([1]) list([0]) list([1, 1]) ... list([]) list([]) list([])] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = utils.Parser(config_file_path=config_file_path).parse()\n",
    "\n",
    "# Set random seeds\n",
    "random_seed = random_seed\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "X_train_embed_masked, y_train_lm_masked, y_train_masked, \\\n",
    "\tX_test_embed, y_test, training_labels_present, \\\n",
    "\tsample_weights_masked, proba_preds_masked = train_downstream_model.load_data(args)\n",
    " \n",
    "print('X_train_embed_masked', X_train_embed_masked,\"\\n\")\n",
    "print('y_train_lm_masked', y_train_lm_masked, \"\\n\")\n",
    "print('y_train_masked', y_train_masked, \"\\n\")\n",
    "print('X_test_embed', X_test_embed, \"\\n\")\n",
    "print('y_test', y_test, \"\\n\")\n",
    "\n",
    "\n",
    "# Train a downstream classifier\n",
    "\n",
    "if args['use_custom_encoder']:\n",
    "\tencoder = models.CustomEncoder(pretrained_model_name_or_path=args['base_encoder'], device=args['device'])\n",
    "else:\n",
    "\tencoder = models.Encoder(model_name=args['base_encoder'], device=args['device'])\n",
    "\n",
    "classifier = models.FeedForwardFlexible(encoder_model=encoder,\n",
    "\t\t\t\t\t\t\t\t\t\th_sizes=args['h_sizes'], \n",
    "\t\t\t\t\t\t\t\t\t\tactivation=eval(args['activation']),\n",
    "\t\t\t\t\t\t\t\t\t\tdevice=torch.device(args['device']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12918349,  0.14052801, -0.04047182, ...,  0.12626587,\n",
       "        -0.07761192, -0.13966799],\n",
       "       [ 0.05691513, -0.00282671, -0.06619749, ...,  0.0996549 ,\n",
       "        -0.04426401, -0.0915379 ],\n",
       "       [ 0.07235474, -0.13375334,  0.03174148, ...,  0.15466037,\n",
       "        -0.08433446, -0.0442021 ],\n",
       "       ...,\n",
       "       [ 0.07823674,  0.02514957,  0.02553626, ...,  0.14149691,\n",
       "        -0.05986933, -0.1253306 ],\n",
       "       [-0.02489712, -0.02762321,  0.08708651, ...,  0.11298889,\n",
       "        -0.00302183, -0.02341386],\n",
       "       [-0.13798836,  0.13346711,  0.03419797, ...,  0.0331003 ,\n",
       "        -0.28610492,  0.12628438]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embed_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training the downstream classifier =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/20 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Training the downstream classifier =====\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_embed_masked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_lm_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights_masked\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_noise_aware_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend_model_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend_model_batch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcriterion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend_model_lr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend_model_weight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend_model_patience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DeepLearning/Final/KeyClassReproducibility/tutorials/../keyclass/train_classifier.py:119\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X_train, y_train, device, sample_weights, epochs, batch_size, criterion, raw_text, lr, weight_decay, patience)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_text \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m: batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    118\u001b[0m out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch_x, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m'\u001b[39m, raw_text\u001b[38;5;241m=\u001b[39mraw_text)\n\u001b[0;32m--> 119\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     batch_weight \u001b[38;5;241m=\u001b[39m sample_weights[indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/keyclass/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/keyclass/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/keyclass/lib/python3.8/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/keyclass/lib/python3.8/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "print('\\n===== Training the downstream classifier =====\\n')\n",
    "model = train_classifier.train(model=classifier, \n",
    "\t\t\t\t\t\t\tdevice=torch.device(args['device']),\n",
    "\t\t\t\t\t\t\tX_train=X_train_embed_masked, \n",
    "\t\t\t\t\t\t\ty_train=y_train_lm_masked,\n",
    "\t\t\t\t\t\t\tsample_weights=sample_weights_masked if args['use_noise_aware_loss'] else None, \n",
    "\t\t\t\t\t\t\tepochs=args['end_model_epochs'], \n",
    "\t\t\t\t\t\t\tbatch_size=args['end_model_batch_size'], \n",
    "\t\t\t\t\t\t\tcriterion=eval(args['criterion']), \n",
    "\t\t\t\t\t\t\traw_text=False, \n",
    "\t\t\t\t\t\t\tlr=eval(args['end_model_lr']), \n",
    "\t\t\t\t\t\t\tweight_decay=eval(args['end_model_weight_decay']),\n",
    "\t\t\t\t\t\t\tpatience=args['end_model_patience'])\n",
    "\n",
    "\n",
    "# end_model_preds_train = model.predict_proba(torch.from_numpy(X_train_embed_masked), batch_size=512, raw_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m end_model_preds_test \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_proba(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(X_test_embed), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, raw_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "end_model_preds_test = model.predict_proba(torch.from_numpy(X_test_embed), batch_size=512, raw_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='self'></a>\n",
    "### 3.2 Self-Training the Model\n",
    "Finally, KeyClass self-trains the downstream model-encoder combination on the entire training dataset to refine the end model classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfetch_data(dataset\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m], path\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m'\u001b[39m], split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m X_test_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfetch_data(dataset\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m], path\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m'\u001b[39m], split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m train_classifier\u001b[38;5;241m.\u001b[39mself_train(model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m, \n\u001b[1;32m      6\u001b[0m \t\t\t\t\t\t\t\t\tX_train\u001b[38;5;241m=\u001b[39mX_train_text, \n\u001b[1;32m      7\u001b[0m \t\t\t\t\t\t\t\t\tX_val\u001b[38;5;241m=\u001b[39mX_test_text, \n\u001b[1;32m      8\u001b[0m \t\t\t\t\t\t\t\t\ty_val\u001b[38;5;241m=\u001b[39my_test, \n\u001b[1;32m      9\u001b[0m \t\t\t\t\t\t\t\t\tdevice\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[1;32m     10\u001b[0m \t\t\t\t\t\t\t\t\tlr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28meval\u001b[39m(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_train_lr\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[1;32m     11\u001b[0m \t\t\t\t\t\t\t\t\tweight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28meval\u001b[39m(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_train_weight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     12\u001b[0m \t\t\t\t\t\t\t\t\tpatience\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_train_patience\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     13\u001b[0m \t\t\t\t\t\t\t\t\tbatch_size\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_train_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     14\u001b[0m \t\t\t\t\t\t\t\t\tq_update_interval\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_update_interval\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     15\u001b[0m \t\t\t\t\t\t\t\t\tself_train_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28meval\u001b[39m(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_train_thresh\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[1;32m     16\u001b[0m \t\t\t\t\t\t\t\t\tprint_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m end_model_preds_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_text, batch_size\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_train_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], raw_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Fetching the raw text data for self-training\n",
    "X_train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n",
    "X_test_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='test')\n",
    "\n",
    "model = train_classifier.self_train(model=model, \n",
    "\t\t\t\t\t\t\t\t\tX_train=X_train_text, \n",
    "\t\t\t\t\t\t\t\t\tX_val=X_test_text, \n",
    "\t\t\t\t\t\t\t\t\ty_val=y_test, \n",
    "\t\t\t\t\t\t\t\t\tdevice=torch.device(args['device']), \n",
    "\t\t\t\t\t\t\t\t\tlr=eval(args['self_train_lr']), \n",
    "\t\t\t\t\t\t\t\t\tweight_decay=eval(args['self_train_weight_decay']),\n",
    "\t\t\t\t\t\t\t\t\tpatience=args['self_train_patience'], \n",
    "\t\t\t\t\t\t\t\t\tbatch_size=args['self_train_batch_size'], \n",
    "\t\t\t\t\t\t\t\t\tq_update_interval=args['q_update_interval'],\n",
    "\t\t\t\t\t\t\t\t\tself_train_thresh=eval(args['self_train_thresh']), \n",
    "\t\t\t\t\t\t\t\t\tprint_eval=True)\n",
    "\n",
    "\n",
    "end_model_preds_test = model.predict_proba(X_test_text, batch_size=args['self_train_batch_size'], raw_text=True)\n",
    "\n",
    "\n",
    "# Print statistics\n",
    "testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n",
    "print(testing_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exp_testing'></a>\n",
    "## 4. Experimentation: Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    3.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_metrics after self train [[0.8847056  0.00193866]\n",
      " [0.88694248 0.00189006]\n",
      " [0.8847056  0.00193866]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    4.6s finished\n"
     ]
    }
   ],
   "source": [
    "end_model_path='../models/imdb/end_model_18-Apr-2024-11_25_44.pth'\n",
    "end_model_self_trained_path='../models/imdb/end_model_self_trained_18 Apr 2024 12:25:05.pth'\n",
    "\n",
    "args = utils.Parser(config_file_path=config_file_path).parse()\n",
    "\n",
    "# Set random seeds\n",
    "random_seed = random_seed\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "X_train_embed_masked, y_train_lm_masked, y_train_masked, \\\n",
    "\tX_test_embed, y_test, training_labels_present, \\\n",
    "\tsample_weights_masked, proba_preds_masked = train_downstream_model.load_data(args)\n",
    "\n",
    "model = torch.load(end_model_path)\n",
    "\n",
    "end_model_preds_train = model.predict_proba(torch.from_numpy(X_train_embed_masked), batch_size=512, raw_text=False)\n",
    "end_model_preds_test = model.predict_proba(torch.from_numpy(X_test_embed), batch_size=512, raw_text=False)\n",
    "\n",
    "# Print statistics\n",
    "if training_labels_present:\n",
    "\ttraining_metrics_with_gt = utils.compute_metrics(y_preds=np.argmax(end_model_preds_train, axis=1), \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_train_masked, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'])\n",
    "\tprint('training_metrics_with_gt', training_metrics_with_gt)\n",
    "\n",
    "training_metrics_with_lm = utils.compute_metrics(y_preds=np.argmax(end_model_preds_train, axis=1), \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_train_lm_masked, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'])\n",
    "print('training_metrics_with_lm', training_metrics_with_lm)\n",
    "\n",
    "testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1), \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n",
    "print('testing_metrics', testing_metrics)\n",
    "\n",
    "\n",
    "print('\\n===== Self-training the downstream classifier =====\\n')\n",
    "\n",
    "# Fetching the raw text data for self-training\n",
    "X_train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n",
    "X_test_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='test')\n",
    "\n",
    "model = torch.load(end_model_self_trained_path)\n",
    "\n",
    "end_model_preds_test = model.predict_proba(X_test_text, batch_size=args['self_train_batch_size'], raw_text=True)\n",
    "\n",
    "\n",
    "# Print statistics\n",
    "testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n",
    "print('testing_metrics after self train', testing_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## 5. References \n",
    "\n",
    "[[1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3270933/)] Nir Menachemi and Taleah H Collum. Benefits and drawbacks of electronic health record systems. Risk management and healthcare policy, 4:47, 2011.\n",
    "\n",
    "[[2](https://academic.oup.com/jamia/article/24/6/1142/4091350)] Julia Adler-Milstein, A Jay Holmgren, Peter Kralovec, Chantal Worzala, Talisha Searcy, and Vaishali Patel. Electronic health record adoption in us hospitals: the emergence of a digital â€œadvanced useâ€ divide. Journal of the American Medical Informatics Association, 24(6):1142â€“1148, 2017.\n",
    "\n",
    "[[3](https://www.tandfonline.com/doi/full/10.1080/2331205X.2021.1893422)] Musaed Ali Alharbi, Godfrey Isouard, and Barry Tolchard. Historical development of the statistical classification of causes of death and diseases. Cogent Medicine, 8(1):1893422, 2021. doi: 10.1080/2331205X.2021.1893422. URL https://doi.org/10.1080/2331205X.2021.1893422.\n",
    "\n",
    "[[4](https://n.neurology.org/content/49/3/660.short)] Curtis Benesch, DM Witter, AL Wilder, PW Duncan, GP Samsa, and DB Matchar. Inaccuracy of the international classification of diseases (icd-9-cm) in identifying the diagnosis of ischemic cerebrovascular disease. Neurology, 49(3):660â€“664, 1997.\n",
    "\n",
    "[[5](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0234647)] Guhan Ram Venkataraman, Arturo Lopez Pineda, Oliver J Bear Donâ€™t Walk IV, Ashley M Zehnder, Sandeep Ayyar, Rodney L Page, Carlos D Bustamante, and Manuel A Rivas. Fastag: Automatic text classification of unstructured medical narratives. PLoS one, 15(6):e0234647, 2020.\n",
    "\n",
    "[[6](https://www.aaai.org/ocs/index.php/WS/AAAIW18/paper/view/16881/0)] Tal Baumel, Jumana Nassour-Kassis, Raphael Cohen, Michael Elhadad, and No Ìemie El- hadad. Multi-label classification of patient notes: case study on icd code assignment. In Workshops at the thirty-second AAAI conference on artificial intelligence, 2018.\n",
    "\n",
    "[[7]()] Sepp Hochreiter and J Ìˆurgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735â€“1780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.\n",
    "\n",
    "[[8](https://link.springer.com/chapter/10.1007/978-3-319-21843-4_12)] Stefano Giovanni Rizzo, Danilo Montesi, Andrea Fabbri, and Giulio Marchesini. Icd code retrieval: Novel approach for assisted disease classification. In International Conference on Data Integration in the Life Sciences, pages 147â€“161. Springer, 2015.\n",
    "\n",
    "[[9](https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-259.pdf?ref=https://githubhelp.com)] Evgeniy Gabrilovich, Shaul Markovitch, et al. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In IJcAI, volume 7, pages 1606â€“1611, 2007.\n",
    "\n",
    "[[10](https://arxiv.org/abs/2010.07245)] Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and Jiawei Han. Text classification using label names only: A language model self-training approach. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro- cessing, 2020.\n",
    "\n",
    "[[11](https://link.springer.com/article/10.1186/s12911-018-0723-6)] Yanshan Wang, Sunghwan Sohn, Sijia Liu, Feichen Shen, Liwei Wang, Elizabeth J Atkinson, Shreyasee Amin, and Hongfang Liu. A clinical text classification paradigm using weak supervision and deep representation. BMC medical informatics and decision making, 19(1):1â€“13, 2019.\n",
    "\n",
    "[[12](https://www.sciencedirect.com/science/article/pii/S0022395621000637)] Marika Cusick, Prakash Adekkanattu, Thomas R Campion Jr, Evan T Sholle, Annie Myers, Samprit Banerjee, George Alexopoulos, Yanshan Wang, and Jyotishman Pathak. Using weak supervision and deep learning to classify clinical notes for identification of current suicidal ideation. Journal of psychiatric research, 136:95â€“102, 2021.\n",
    "\n",
    "[[13](https://proceedings.neurips.cc/paper/2016/hash/6709e8d64a5f47269ed5cea9f625f7ab-Abstract.html)] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R Ìe. Data programming: Creating large training sets, quickly. In Advances in neural infor- mation processing systems, pages 3567â€“3575, 2016.\n",
    "\n",
    "[[14](https://arxiv.org/abs/1810.04805)] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171â€“ 4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "318bfa02e7908bdce70328a8696b89c0eda4a9244e2fc5762a68deeb8a3ec1f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
