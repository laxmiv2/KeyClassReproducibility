{"cells":[{"cell_type":"markdown","metadata":{"id":"bowfc2gY9LN0"},"source":["\n","\n","# üìñ KeyClass Tutorial: Text Classification with Label-Descriptions Only\n","\n","\n","<hr>\n","\n","\n","***Author(s):*** Arnab Dey, Chufan Gao, Mononito Goswami, correspondence to &lt;mgoswami@andrew.cmu.edu&gt;\n","\n","<img align=\"right\" src=\"../assets/autonlab_logo.png\" width=\"20%\"/>\n","\n","## Contents\n","\n","\n","### 1. [Problem Background & Motivation](#introduction)\n","\n","####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  1.1 [Electronic Health Records (EHR)](#ehr)\n","####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  1.2 [Generalizable Insights in Healthcare Contexts](#insights)\n","\n","\n","### 2. [Methodology](#methodology)\n","\n","####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.1 [Prior Work](#prior)\n","####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.2 [KeyClass](#keyclass)\n","<!-- ####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.3 [Problem Formulation](#math) -->\n","####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.3 [Find Class Descriptions](#classdesc)\n","####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.4 [Find Relevant Keywords](#keywords)\n","####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  2.5 [Probabilistically Labeling the Data](#label)\n","\n","\n","### 3. [Experimentation: Training](#exp_training)\n","\n","####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  3.1 [Training the Downstream Model](#downstream)\n","####    &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;  3.2 [Self-Training the Model](#self)\n","\n","### 4. [Experimentation: Testing](#exp_testing)\n","\n","### 5. [References](#references)\n","\n","<hr>\n"]},{"cell_type":"markdown","metadata":{"id":"UC4n0UBb9LN2"},"source":["<a id='introduction'></a>\n","## 1. Problem Background & Motivation"]},{"cell_type":"markdown","metadata":{"id":"4AxfMg_h9LN2"},"source":["<a id='ehr'></a>\n","### 1.1 Electronic Health Records (EHR)\n","\n","The Electronic Health Record __(EHR)__ system is a digital version of a patient‚Äôs paper chart. __EHRs__ are almost-real-time, patient-centered records that contain `patient history`, `diagnoses`, `procedures`, `medications`, and more in an easily accessible format. Since the _Health Information Technology for Economic and Clinical Health_ <b>(HITECH)</b> Act was signed into law in 2009, adoption rates of these systems have steadily increased<sup><a href=\"#references\"><b>1</b></a></sup>. Adler-Milstein et al.<sup><a href=\"#references\"><b>2</b></a></sup>, who analyzed survey data collected by American Hospital Association found that EHR adoption rates were at <code><b>80%</b></code> in __2017__, twice the rate in __2008__. With higher adoption rates comes the rising challenge of _data processing and analysis of unstructured clinical text._ Due to the unstructured nature of clinical notes, providers often employ trained staff and/or third-party vendors to help assign diagnostic codes using coding systems such as the International Classification of Diseases __(ICD)__<sup><a href=\"#references\"><b>3</b></a></sup>.\n","\n","__However, manual assignment of ICD codes is problematic:__\n","1. It is both time consuming and error-prone, with only <code><b>60-80%</b></code> of the assigned codes reflecting actual patient diagnoses<sup><a href=\"#references\"><b>4</b></a></sup>\n","1. A significant portion of code assignment results in misjudged severity of conditions and code omissions<sup><a href=\"#references\"><b>5</b></a></sup>\n","1. For healthcare providers, billing and coding errors may not only lead to loss of revenue and claim denials, but also federal penalties for erroneous Medicare and Medicaid claims\n","\n","Thus, there is a clear need for reliable automated classification of unstructured clinical notes."]},{"cell_type":"markdown","metadata":{"id":"29wXJGFR9LN2"},"source":["<a id='insights'></a>\n","### 1.2 Generalizable Insights in Healthcare Contexts\n","\n","Managing costs and quality of healthcare is a persistent societal challenge of enormous magnitude and impact on daily lives of all people. Our approach proposes a low-cost solution that has the potential to address some of the identified pressing issues with accessibility to affordable yet accurate automated disease coding tools. Our contributions lie in using a novel strategy\n","to efficiently acquire interpretable weak supervision sources from readily available text to learn effective text classifiers without the need for human-labeled data.\n","\n","__Our work demonstrates:__\n","1. Pre-trained language models can efficiently and effectively inform weakly supervised models for text classification\n","1. Self-training improves downstream classifier performance, especially when classifiers are initially trained on a subset of the training data\n","1. Data programming performs on par with simple majority vote when relying on a large number of automatically generated weak supervision sources of similar quality\n","1. Keywords are excellent sources of weak supervision"]},{"cell_type":"markdown","metadata":{"id":"Mq_s60ah9LN3"},"source":["<a id='methodology'></a>\n","## 2. Methodology\n","\n","<!-- <center> -->\n","<img align=\"top\" src=\"../assets/KeyClass.png\" width=\"50%\"/>\n","<!-- </center> -->\n","\n","<b>Figure A:</b> Overview of our methodology. From only class descriptions, KeyClass classifies documents without access to any labeled data. It automatically creates interpretable labeling functions (LFs) by extracting frequent keywords and phrases that are highly indicative of a particular class from the unlabeled text using a pre-trained language model. It then uses these LFs along with Data Programming (DP) to generate probabilistic labels for training data, which are used to train a downstream classifier <sup><a href=\"#references\"><b>13</b></a></sup>."]},{"cell_type":"markdown","metadata":{"id":"Yivoaeb_9LN3"},"source":["<a id='prior'></a>\n","### 2.1 Prior Work\n","\n","__Assigning ICD codes to Clinical Notes<sup><a href=\"#references\"><b>[6,7,8]</b></a></sup>:__\n","1. To the best of our knowledge, all prior work on ICD code assignment utilized __fully supervised ML techniques__, most of them relying on vast quantities of labeled training data\n","1. In this work, we explore the use of our proposed __weakly supervised model _KeyClass___ to assign top-level `ICD-9` codes to long patient discharge summaries\n","1. Its training signal is retrieved automatically from readily available descriptions of the ICD codes, therefore it requires no human-produced supervisory feedback to build effective downstream text classifiers\n","\n","__Text Classification with Sparse Training Labels<sup><a href=\"#references\"><b>[9,10]</b></a></sup>:__\n","1. Our work differs from prior work because the foundation of our weak supervision methodology, i.e., frequent keywords and phrases as LFs, is highly interpretable\n","1. Secondly, while previously proposed state-of-the-art models are committed to specific language model architectures for linguistic knowledge and representation learning, KeyClass offers a high degree of modularity, enabling end users to adapt the neural language model (encoder) and downstream classifiers to specific problems, such as clinical text classification\n","1. Finally, we explore the use of weak supervision for multilabel multiclass classification, a problem which, to the best of our knowledge, has not been tackled by prior work on weak text classification\n","\n","__Weak Supervision for Clinical Text Classification<sup><a href=\"#references\"><b>[11,12]</b></a></sup>:__\n","1. Prior work on weakly supervised clinical text classification had an explicit dependence on manually created rule-based labeling functions\n","1. In this work, however, we demonstrate that we can quickly and automatically create simple keyword based labeling functions, with minimal to no human involvement"]},{"cell_type":"markdown","metadata":{"id":"1k-p_-MN9LN3"},"source":["<a id='keyclass'></a>\n","### 2.2 KeyClass\n","\n","As a potential remedy, we present KeyClass, a general weakly supervised text classification framework combining Data Programming<sup><a href=\"#references\"><b>13</b></a></sup> with a novel method of automatically acquiring interpretable weak supervision sources (keywords and phrases) from class-label descriptions only without the need to access to any labeled documents. The successful application of KeyClass to solve an important clinical text classification problem demonstrates its potential for making social impact by allowing quick and affordable development and deployment of effective text classifiers.\n","\n","<img align=\"top\" src=\"../assets/flowchart.png\" width=\"50%\"/>\n","\n","<b>Figure B:</b> Data programming, or weak supervision compared to fully supervised ML. The orange boxes indicate the effort required by expert annotators. Instead of having to label extensive quantities of data by hand, the effort in data programming framework lies in obtaining labeling functions. In KeyClass, these labeling functions are our keyword-matching rules automatically extracted from reference data, to further reduce required human effort."]},{"cell_type":"markdown","metadata":{"id":"59JyqDfy9LN3"},"source":["<a id='classdesc'></a>\n","### 2.3 Find Class Descriptions\n","Unlike traditional supervised learning where each document needs to be labeled, KeyClass only relies on meaningful and succinct class descriptions, also removing the requirement of expert heuristics as in prior weak supervision work. As a concrete example, let's consider the IMDb movie review sentiment classification problem, where the objective is to classify a movie re-\n","view as being `positive` or `negative`. In order to initiate the classification process, domain experts provide <code><b>KeyClass</b></code> with common sense descriptions of a __positive__ (`good amazing exciting positive`) and __negative review__ (`terrible bad boring negative`). In most cases, these descriptions can be automatically generated from Wikipedia articles or reference manuals and validated by domain experts, further reducing manual effort. Class Descriptions used in this tutorial can be found [here](./config_files/config_imdb.yml)"]},{"cell_type":"markdown","metadata":{"id":"R1StYtXU9LN3"},"source":["<a id='keywords'></a>\n","### 2.4 Find Relevant Keywords / Encoding the Dataset\n","\n","Once we have the class descriptions, KeyClass automatically discovers highly suggestive keywords and phrases for each class. KeyClass first obtains frequent n-grams from the training corpus to serve as keywords or key-phrases for its automatically composed labeling functions. In order to transform the keywords into labeling functions of the prescribed form, KeyClass leverage the general linguistic knowledge stored within pre-trained neural language models such as Bidirectional Encoder Representations from Transformers __(BERT)__<sup><a href=\"#references\"><b>14</b></a></sup>, to map each keyword to the most semantically related category description. To create a labeling function, KeyClass simply assigns a keyword to its closest category as measured by the cosine similarity between their embeddings. In order to ensure equal representation of all classes, KeyClass sub-samples the top-k labeling functions per class, ordering them by cosine similarity. While theoretically data programming benefits from as many labeling functions as possible, the sampling is required due to computational and space constraints."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pr6BMZoN9LN3"},"outputs":[],"source":["import sys\n","sys.path.append('../keyclass/')\n","sys.path.append('../scripts/')\n","\n","import argparse\n","import label_data, encode_datasets, train_downstream_model\n","import torch\n","import pickle\n","import numpy as np\n","import os\n","from os.path import join, exists\n","from datetime import datetime\n","import utils\n","import models\n","import create_lfs\n","import train_classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-TzmCPg9LN4"},"outputs":[],"source":["# Input arguments\n","config_file_path = r'../config_files/config_mimic.yml' # Specify path to the configuration file\n","random_seed = 0 # Random seed for experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4o3OzDBx9LN4","outputId":"b9cf7c4e-ea97-4f3e-f3bc-d58c0bf5f2b5","colab":{"referenced_widgets":["8c9116e026914c50bea28730c962da18","ee7752b406204a31951762a506bbeed2"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c9116e026914c50bea28730c962da18","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/5931 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee7752b406204a31951762a506bbeed2","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1318 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["args = utils.Parser(config_file_path=config_file_path).parse()\n","# model_name = \"bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16\"\n","# model = AutoModel.from_pretrained(model_name)\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","\n","if args['use_custom_encoder']:\n","    model = models.CustomEncoder(pretrained_model_name_or_path=args['base_encoder'],\n","        device='cuda' if torch.cuda.is_available() else 'cpu')\n","else:\n","    model = models.Encoder(model_name=args['base_encoder'],\n","        device='cuda' if torch.cuda.is_available() else 'cpu')\n","\n","for split in ['train', 'test']:\n","    sentences = utils.fetch_data(dataset=args['dataset'], split=split, path=args['data_path'])\n","    embeddings = model.encode(sentences=sentences, batch_size=args['end_model_batch_size'],\n","                                show_progress_bar=args['show_progress_bar'],\n","                                normalize_embeddings=args['normalize_embeddings'])\n","    with open(join(args['data_path'], args['dataset'], f'{split}_embeddings.pkl'), 'wb') as f:\n","        pickle.dump(embeddings, f)"]},{"cell_type":"markdown","metadata":{"id":"XUFs10ox9LN5"},"source":["<a id='label'></a>\n","### 2.5 Probabilistically Labeling the Data\n","\n","Next, _KeyClass_ constructs the labeling function vote matrix and generates probabilistic labels for all training documents using a label model. Specifically, we use the open-source label model implementation of the ___Snorkel Python library___<sup><a href=\"#references\"><b>13</b></a></sup>."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56BFDdvI9LN5","outputId":"c159ffeb-66f9-446e-aed3-ccd2d2fff89f"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load training data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_text \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m training_labels_present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(join(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m'\u001b[39m], args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_labels.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n","File \u001b[0;32m~/Documents/DeepLearning/Final/KeyClassReproducibility/tutorials/../keyclass/utils.py:221\u001b[0m, in \u001b[0;36mfetch_data\u001b[0;34m(dataset, path, split)\u001b[0m\n\u001b[1;32m    218\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjoin(path,\u001b[38;5;250m \u001b[39mdataset,\u001b[38;5;250m \u001b[39msplit)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmimic\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 221\u001b[0m     text \u001b[38;5;241m=\u001b[39m [cleantext\u001b[38;5;241m.\u001b[39mclean(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n","File \u001b[0;32m~/Documents/DeepLearning/Final/KeyClassReproducibility/tutorials/../keyclass/utils.py:221\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    218\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjoin(path,\u001b[38;5;250m \u001b[39mdataset,\u001b[38;5;250m \u001b[39msplit)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmimic\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 221\u001b[0m     text \u001b[38;5;241m=\u001b[39m [\u001b[43mcleantext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n","File \u001b[0;32m~/anaconda3/envs/keyclass/lib/python3.8/site-packages/cleantext/cleantext.py:63\u001b[0m, in \u001b[0;36mclean\u001b[0;34m(text, clean_all, extra_spaces, stemming, stopwords, lowercase, numbers, punct, reg, reg_replace, stp_lang)\u001b[0m\n\u001b[1;32m     61\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([_ \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _\u001b[38;5;241m.\u001b[39misdigit()])\n\u001b[1;32m     62\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m---> 63\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([ps\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens\n\u001b[1;32m     64\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words])\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_spaces:\n","File \u001b[0;32m~/anaconda3/envs/keyclass/lib/python3.8/site-packages/cleantext/cleantext.py:64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([_ \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _\u001b[38;5;241m.\u001b[39misdigit()])\n\u001b[1;32m     62\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     63\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([ps\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens\n\u001b[0;32m---> 64\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m])\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_spaces:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from transformers import AutoModel, AutoTokenizer\n","\n","# Load training data\n","train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n","\n","training_labels_present = False\n","if exists(join(args['data_path'], args['dataset'], 'train_labels.txt')):\n","    with open(join(args['data_path'], args['dataset'], 'train_labels.txt'), 'r') as f:\n","        y_train = f.readlines()\n","    y_train = np.array([int(i.replace('\\n','')) for i in y_train])\n","    training_labels_present = True\n","else:\n","    y_train = None\n","    training_labels_present = False\n","    print('No training labels found!')\n","\n","with open(join(args['data_path'], args['dataset'], 'train_embeddings.pkl'), 'rb') as f:\n","    X_train = pickle.load(f)\n","\n","# Print dataset statistics\n","print(f\"Getting labels for the {args['dataset']} data...\")\n","print(f'Size of the data: {len(train_text)}')\n","if training_labels_present:\n","    print('Class distribution', np.unique(y_train, return_counts=True))\n","\n","# Load label names/descriptions\n","label_names = []\n","for a in args:\n","    if 'target' in a: label_names.append(args[a])\n","\n","\n","# model_name = \"bluebert_pubmed_uncased_L-24_H-1024_A-16\"\n","# model = AutoModel.from_pretrained(model_name)\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Creating labeling functions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jrvrzl3i9LN5","outputId":"8872fe10-5271-44a8-f3ed-ffc9ab921d2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using custom encoder\n"]}],"source":["labeler = create_lfs.CreateLabellingFunctions(base_encoder=\"bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12\",\n","                                            device=torch.device(args['device']),\n","                                            label_model=args['label_model'], custom_encoder=args['use_custom_encoder'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkiE7PGd9LN5","outputId":"308dd901-04c1-41a9-ce19-79191d9a1c69"},"outputs":[{"ename":"NameError","evalue":"name 'train_text' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m proba_preds \u001b[38;5;241m=\u001b[39m labeler\u001b[38;5;241m.\u001b[39mget_labels(text_corpus\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_text\u001b[49m, label_names\u001b[38;5;241m=\u001b[39mlabel_names, min_df\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_df\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      2\u001b[0m                                 ngram_range\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mngram_range\u001b[39m\u001b[38;5;124m'\u001b[39m], topk\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopk\u001b[39m\u001b[38;5;124m'\u001b[39m], y_train\u001b[38;5;241m=\u001b[39my_train, \n\u001b[1;32m      3\u001b[0m                                 label_model_lr\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_model_lr\u001b[39m\u001b[38;5;124m'\u001b[39m], label_model_n_epochs\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_model_n_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      4\u001b[0m                                 verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_classes\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_classes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'train_text' is not defined"]}],"source":["proba_preds = labeler.get_labels(text_corpus=train_text, label_names=label_names, min_df=args['min_df'],\n","                                ngram_range=args['ngram_range'], topk=args['topk'], y_train=y_train,\n","                                label_model_lr=args['label_model_lr'], label_model_n_epochs=args['label_model_n_epochs'],\n","                                verbose=True, n_classes=args['n_classes'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwxjc8QJ9LN5","outputId":"3b1bc82e-17ff-4e40-eea6-bf4152327751"},"outputs":[{"ename":"NameError","evalue":"name 'proba_preds' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mproba_preds\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'proba_preds' is not defined"]}],"source":["y_train_pred = np.argmax(proba_preds, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgWjRRHo9LN5","outputId":"ab9356bd-2d3b-4dde-afa2-32d4c9a3d940"},"outputs":[{"ename":"NameError","evalue":"name 'proba_preds' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds_path\u001b[39m\u001b[38;5;124m'\u001b[39m]): os\u001b[38;5;241m.\u001b[39mmakedirs(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(join(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_model\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_proba_preds.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mproba_preds\u001b[49m, f)\n","\u001b[0;31mNameError\u001b[0m: name 'proba_preds' is not defined"]}],"source":["# Save the predictions\n","if not os.path.exists(args['preds_path']): os.makedirs(args['preds_path'])\n","with open(join(args['preds_path'], f\"{args['label_model']}_proba_preds.pkl\"), 'wb') as f:\n","    pickle.dump(proba_preds, f)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZiPxrnv9LN5"},"outputs":[],"source":["with open(join(args['data_path'], args['dataset'], 'train_labels.txt'), 'w') as f:\n","    for item in y_train_pred:\n","        print(item)\n","        f.write(\"%s\\n\" % item)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ab4hk3k9LN5","outputId":"32f4ef97-f2c8-44b8-802a-f13c930f1388"},"outputs":[{"ename":"NameError","evalue":"name 'y_train_pred' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel Model Predictions: Unique value and counts\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(\u001b[43my_train_pred\u001b[49m, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_labels_present:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel Model Training Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(y_train_pred\u001b[38;5;241m==\u001b[39my_train))\n","\u001b[0;31mNameError\u001b[0m: name 'y_train_pred' is not defined"]}],"source":["# Print statistics\n","print('Label Model Predictions: Unique value and counts', np.unique(y_train_pred, return_counts=True))\n","if training_labels_present:\n","    print('Label Model Training Accuracy', np.mean(y_train_pred==y_train))\n","\n","    # Log the metrics\n","    training_metrics_with_gt = utils.compute_metrics(y_preds=y_train_pred, y_true=y_train, average=args['average'])\n","    utils.log(metrics=training_metrics_with_gt, filename='label_model_with_ground_truth',\n","        results_dir=args['results_path'], split='train')"]},{"cell_type":"markdown","metadata":{"id":"mqU6zFcx9LN6"},"source":["<a id='exp_training'></a>\n","## 3. Experimentation: Training"]},{"cell_type":"markdown","metadata":{"id":"p_EeDloX9LN6"},"source":["<a id='downstream'></a>\n","### 3.1 Training the Downstream Model\n","\n","After obtaining a probabilistically labeled training dataset, KeyClass can train any downstream classifier using rich document feature representations provided by the neural language model. Instead of using all the automatically labeled documents, KeyClass initially trains the downstream classifier using top-$k$ documents with the most confident label estimates only."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-b9SGzzL9LN6","outputId":"2fde137b-b664-4c39-ac5a-a07e2fe70dbd"},"outputs":[{"ename":"EOFError","evalue":"Ran out of input","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(random_seed)\n\u001b[1;32m      6\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(random_seed)\n\u001b[1;32m      8\u001b[0m X_train_embed_masked, y_train_lm_masked, y_train_masked, \\\n\u001b[1;32m      9\u001b[0m \tX_test_embed, y_test, training_labels_present, \\\n\u001b[0;32m---> 10\u001b[0m \tsample_weights_masked, proba_preds_masked \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_downstream_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train a downstream classifier\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_custom_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n","File \u001b[0;32m~/Documents/DeepLearning/Final/KeyClassReproducibility/tutorials/../scripts/train_downstream_model.py:54\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(args):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m     52\u001b[0m             join(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_model\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_proba_preds.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 54\u001b[0m         proba_preds \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# y_train_lm = np.argmax(proba_preds, axis=1)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     y_train_lm \u001b[38;5;241m=\u001b[39m (proba_preds\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n","\u001b[0;31mEOFError\u001b[0m: Ran out of input"]}],"source":["args = utils.Parser(config_file_path=config_file_path).parse()\n","\n","# Set random seeds\n","random_seed = random_seed\n","torch.manual_seed(random_seed)\n","np.random.seed(random_seed)\n","\n","X_train_embed_masked, y_train_lm_masked, y_train_masked, \\\n","\tX_test_embed, y_test, training_labels_present, \\\n","\tsample_weights_masked, proba_preds_masked = train_downstream_model.load_data(args)\n","\n","# Train a downstream classifier\n","\n","if args['use_custom_encoder']:\n","\tencoder = models.CustomEncoder(pretrained_model_name_or_path=args['base_encoder'], device=args['device'])\n","else:\n","\tencoder = models.Encoder(model_name=args['base_encoder'], device=args['device'])\n","\n","classifier = models.FeedForwardFlexible(encoder_model=encoder,\n","\t\t\t\t\t\t\t\t\t\th_sizes=args['h_sizes'],\n","\t\t\t\t\t\t\t\t\t\tactivation=eval(args['activation']),\n","\t\t\t\t\t\t\t\t\t\tdevice=torch.device(args['device']))\n","print('\\n===== Training the downstream classifier =====\\n')\n","model = train_classifier.train(model=classifier,\n","\t\t\t\t\t\t\tdevice=torch.device(args['device']),\n","\t\t\t\t\t\t\tX_train=X_train_embed_masked,\n","\t\t\t\t\t\t\ty_train=y_train_lm_masked,\n","\t\t\t\t\t\t\tsample_weights=sample_weights_masked if args['use_noise_aware_loss'] else None,\n","\t\t\t\t\t\t\tepochs=args['end_model_epochs'],\n","\t\t\t\t\t\t\tbatch_size=args['end_model_batch_size'],\n","\t\t\t\t\t\t\tcriterion=eval(args['criterion']),\n","\t\t\t\t\t\t\traw_text=False,\n","\t\t\t\t\t\t\tlr=eval(args['end_model_lr']),\n","\t\t\t\t\t\t\tweight_decay=eval(args['end_model_weight_decay']),\n","\t\t\t\t\t\t\tpatience=args['end_model_patience'])\n","\n","\n","end_model_preds_train = model.predict_proba(torch.from_numpy(X_train_embed_masked), batch_size=512, raw_text=False)\n","end_model_preds_test = model.predict_proba(torch.from_numpy(X_test_embed), batch_size=512, raw_text=False)"]},{"cell_type":"markdown","metadata":{"id":"8Iugsxmf9LN6"},"source":["<a id='self'></a>\n","### 3.2 Self-Training the Model\n","Finally, KeyClass self-trains the downstream model-encoder combination on the entire training dataset to refine the end model classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"papfrZCY9LN6"},"outputs":[],"source":["# Fetching the raw text data for self-training\n","X_train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n","X_test_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='test')\n","\n","model = train_classifier.self_train(model=model,\n","\t\t\t\t\t\t\t\t\tX_train=X_train_text,\n","\t\t\t\t\t\t\t\t\tX_val=X_test_text,\n","\t\t\t\t\t\t\t\t\ty_val=y_test,\n","\t\t\t\t\t\t\t\t\tdevice=torch.device(args['device']),\n","\t\t\t\t\t\t\t\t\tlr=eval(args['self_train_lr']),\n","\t\t\t\t\t\t\t\t\tweight_decay=eval(args['self_train_weight_decay']),\n","\t\t\t\t\t\t\t\t\tpatience=args['self_train_patience'],\n","\t\t\t\t\t\t\t\t\tbatch_size=args['self_train_batch_size'],\n","\t\t\t\t\t\t\t\t\tq_update_interval=args['q_update_interval'],\n","\t\t\t\t\t\t\t\t\tself_train_thresh=eval(args['self_train_thresh']),\n","\t\t\t\t\t\t\t\t\tprint_eval=True)\n","\n","\n","end_model_preds_test = model.predict_proba(X_test_text, batch_size=args['self_train_batch_size'], raw_text=True)\n","\n","\n","# Print statistics\n","testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1),\n","\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test,\n","\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'],\n","\t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'],\n","\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n","print(testing_metrics)"]},{"cell_type":"markdown","metadata":{"id":"W-WbJkwW9LN6"},"source":["<a id='exp_testing'></a>\n","## 4. Experimentation: Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKDkMGYh9LN6","outputId":"7c6947e3-68ad-4830-f300-26ce8d2c0eee"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    3.9s\n"]},{"name":"stdout","output_type":"stream","text":["testing_metrics after self train [[0.8847056  0.00193866]\n"," [0.88694248 0.00189006]\n"," [0.8847056  0.00193866]]\n"]},{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    4.6s finished\n"]}],"source":["end_model_path='../models/imdb/end_model_18-Apr-2024-11_25_44.pth'\n","end_model_self_trained_path='../models/imdb/end_model_self_trained_18 Apr 2024 12:25:05.pth'\n","\n","args = utils.Parser(config_file_path=config_file_path).parse()\n","\n","# Set random seeds\n","random_seed = random_seed\n","torch.manual_seed(random_seed)\n","np.random.seed(random_seed)\n","\n","X_train_embed_masked, y_train_lm_masked, y_train_masked, \\\n","\tX_test_embed, y_test, training_labels_present, \\\n","\tsample_weights_masked, proba_preds_masked = train_downstream_model.load_data(args)\n","\n","model = torch.load(end_model_path)\n","\n","end_model_preds_train = model.predict_proba(torch.from_numpy(X_train_embed_masked), batch_size=512, raw_text=False)\n","end_model_preds_test = model.predict_proba(torch.from_numpy(X_test_embed), batch_size=512, raw_text=False)\n","\n","# Print statistics\n","if training_labels_present:\n","\ttraining_metrics_with_gt = utils.compute_metrics(y_preds=np.argmax(end_model_preds_train, axis=1),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_train_masked,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'])\n","\tprint('training_metrics_with_gt', training_metrics_with_gt)\n","\n","training_metrics_with_lm = utils.compute_metrics(y_preds=np.argmax(end_model_preds_train, axis=1),\n","\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_train_lm_masked,\n","\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'])\n","print('training_metrics_with_lm', training_metrics_with_lm)\n","\n","testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1),\n","\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test,\n","\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'],\n","\t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'],\n","\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n","print('testing_metrics', testing_metrics)\n","\n","\n","print('\\n===== Self-training the downstream classifier =====\\n')\n","\n","# Fetching the raw text data for self-training\n","X_train_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='train')\n","X_test_text = utils.fetch_data(dataset=args['dataset'], path=args['data_path'], split='test')\n","\n","model = torch.load(end_model_self_trained_path)\n","\n","end_model_preds_test = model.predict_proba(X_test_text, batch_size=args['self_train_batch_size'], raw_text=True)\n","\n","\n","# Print statistics\n","testing_metrics = utils.compute_metrics_bootstrap(y_preds=np.argmax(end_model_preds_test, axis=1),\n","\t\t\t\t\t\t\t\t\t\t\t\t\ty_true=y_test,\n","\t\t\t\t\t\t\t\t\t\t\t\t\taverage=args['average'],\n","\t\t\t\t\t\t\t\t\t\t\t\t\tn_bootstrap=args['n_bootstrap'],\n","\t\t\t\t\t\t\t\t\t\t\t\t\tn_jobs=args['n_jobs'])\n","print('testing_metrics after self train', testing_metrics)\n"]},{"cell_type":"markdown","metadata":{"id":"F_eYKhcQ9LN6"},"source":["<a id='references'></a>\n","## 5. References\n","\n","[[1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3270933/)] Nir Menachemi and Taleah H Collum. Benefits and drawbacks of electronic health record systems. Risk management and healthcare policy, 4:47, 2011.\n","\n","[[2](https://academic.oup.com/jamia/article/24/6/1142/4091350)] Julia Adler-Milstein, A Jay Holmgren, Peter Kralovec, Chantal Worzala, Talisha Searcy, and Vaishali Patel. Electronic health record adoption in us hospitals: the emergence of a digital ‚Äúadvanced use‚Äù divide. Journal of the American Medical Informatics Association, 24(6):1142‚Äì1148, 2017.\n","\n","[[3](https://www.tandfonline.com/doi/full/10.1080/2331205X.2021.1893422)] Musaed Ali Alharbi, Godfrey Isouard, and Barry Tolchard. Historical development of the statistical classification of causes of death and diseases. Cogent Medicine, 8(1):1893422, 2021. doi: 10.1080/2331205X.2021.1893422. URL https://doi.org/10.1080/2331205X.2021.1893422.\n","\n","[[4](https://n.neurology.org/content/49/3/660.short)] Curtis Benesch, DM Witter, AL Wilder, PW Duncan, GP Samsa, and DB Matchar. Inaccuracy of the international classification of diseases (icd-9-cm) in identifying the diagnosis of ischemic cerebrovascular disease. Neurology, 49(3):660‚Äì664, 1997.\n","\n","[[5](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0234647)] Guhan Ram Venkataraman, Arturo Lopez Pineda, Oliver J Bear Don‚Äôt Walk IV, Ashley M Zehnder, Sandeep Ayyar, Rodney L Page, Carlos D Bustamante, and Manuel A Rivas. Fastag: Automatic text classification of unstructured medical narratives. PLoS one, 15(6):e0234647, 2020.\n","\n","[[6](https://www.aaai.org/ocs/index.php/WS/AAAIW18/paper/view/16881/0)] Tal Baumel, Jumana Nassour-Kassis, Raphael Cohen, Michael Elhadad, and No ÃÅemie El- hadad. Multi-label classification of patient notes: case study on icd code assignment. In Workshops at the thirty-second AAAI conference on artificial intelligence, 2018.\n","\n","[[7]()] Sepp Hochreiter and J Ãàurgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735‚Äì1780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.\n","\n","[[8](https://link.springer.com/chapter/10.1007/978-3-319-21843-4_12)] Stefano Giovanni Rizzo, Danilo Montesi, Andrea Fabbri, and Giulio Marchesini. Icd code retrieval: Novel approach for assisted disease classification. In International Conference on Data Integration in the Life Sciences, pages 147‚Äì161. Springer, 2015.\n","\n","[[9](https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-259.pdf?ref=https://githubhelp.com)] Evgeniy Gabrilovich, Shaul Markovitch, et al. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In IJcAI, volume 7, pages 1606‚Äì1611, 2007.\n","\n","[[10](https://arxiv.org/abs/2010.07245)] Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and Jiawei Han. Text classification using label names only: A language model self-training approach. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro- cessing, 2020.\n","\n","[[11](https://link.springer.com/article/10.1186/s12911-018-0723-6)] Yanshan Wang, Sunghwan Sohn, Sijia Liu, Feichen Shen, Liwei Wang, Elizabeth J Atkinson, Shreyasee Amin, and Hongfang Liu. A clinical text classification paradigm using weak supervision and deep representation. BMC medical informatics and decision making, 19(1):1‚Äì13, 2019.\n","\n","[[12](https://www.sciencedirect.com/science/article/pii/S0022395621000637)] Marika Cusick, Prakash Adekkanattu, Thomas R Campion Jr, Evan T Sholle, Annie Myers, Samprit Banerjee, George Alexopoulos, Yanshan Wang, and Jyotishman Pathak. Using weak supervision and deep learning to classify clinical notes for identification of current suicidal ideation. Journal of psychiatric research, 136:95‚Äì102, 2021.\n","\n","[[13](https://proceedings.neurips.cc/paper/2016/hash/6709e8d64a5f47269ed5cea9f625f7ab-Abstract.html)] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R ÃÅe. Data programming: Creating large training sets, quickly. In Advances in neural infor- mation processing systems, pages 3567‚Äì3575, 2016.\n","\n","[[14](https://arxiv.org/abs/1810.04805)] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171‚Äì 4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"},"vscode":{"interpreter":{"hash":"318bfa02e7908bdce70328a8696b89c0eda4a9244e2fc5762a68deeb8a3ec1f0"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}